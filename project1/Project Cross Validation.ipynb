{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.proj1_helpers import load_csv_data, predict_labels, create_csv_submission, cross_validation_visualization, sort_predictions\n",
    "from utils.preprocessing import adjust_features, nan_to_mean, standardize, split_jets\n",
    "from utils.implementations import ridge_regression, logistic_regression, build_k_indices, cross_validation\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train.csv'\n",
    "test_path = './data/test.csv'\n",
    "OUTPUT = 'jet_pred/prediction.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Datasets loaded in: 23.776541709899902\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Loading datasets\")\n",
    "train_y, train_x, train_ids = load_csv_data(train_path)\n",
    "test_y, test_x, idstest_ids = load_csv_data(test_path)\n",
    "print(\"Datasets loaded in: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jets_train, \\\n",
    "x_jets_test, \\\n",
    "y_jets_train, \\\n",
    "ids = split_jets(train_x, train_y, test_x, test_y, idstest_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jets_train, x_jets_test = adjust_features(x_jets_train, x_jets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 33\n",
    "k_fold = 8\n",
    "jet_to_train = 7\n",
    "degree = 4\n",
    "lambdas = np.logspace(-9, -1, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jets_train[jet_to_train], x_jets_test[jet_to_train] = standardize(x_jets_train[jet_to_train], x_jets_test[jet_to_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_indices = build_k_indices(y_jets_train[jet_to_train], k_fold, seed)\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "best_loss = 50000\n",
    "best_lambda = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iterations = 12542.4982322\n",
      "Loss after 100 iterations = 12931.5009788\n",
      "Loss after 200 iterations = 11348.5881245\n",
      "Loss after 300 iterations = 11013.4454034\n",
      "Loss after 400 iterations = 11302.5287797\n",
      "Loss after 500 iterations = 13022.1582169\n",
      "Loss after 600 iterations = 10915.9797169\n",
      "Loss after 700 iterations = 10425.5586777\n",
      "Loss after 800 iterations = 12742.133632\n",
      "Loss after 900 iterations = 11034.318325\n",
      "Loss after 1000 iterations = 11893.3481169\n",
      "Loss after 1100 iterations = 10489.3460143\n",
      "Loss after 1200 iterations = 10605.0016986\n",
      "Loss after 1300 iterations = 11590.7248905\n",
      "Loss after 1400 iterations = 10904.2436325\n",
      "Loss after 1500 iterations = 11490.6416796\n",
      "Loss after 1600 iterations = 10878.9840757\n",
      "Loss after 1700 iterations = 11234.7508774\n",
      "Loss after 1800 iterations = 11674.1803457\n",
      "Loss after 1900 iterations = 10129.9688206\n",
      "Loss after 2000 iterations = 10048.3046082\n",
      "Loss after 2100 iterations = 11409.7499755\n",
      "Loss after 2200 iterations = 10316.918371\n",
      "Loss after 2300 iterations = 11271.0071719\n",
      "Loss after 2400 iterations = 10520.7639329\n",
      "Loss after 2500 iterations = 10671.3837226\n",
      "Loss after 2600 iterations = 10528.3116555\n",
      "Loss after 2700 iterations = 9601.22141807\n",
      "Loss after 2800 iterations = 10160.6307154\n",
      "Loss after 2900 iterations = 9691.24007795\n",
      "Loss after 3000 iterations = 10676.9117086\n",
      "Loss after 3100 iterations = 10913.3220951\n",
      "Loss after 3200 iterations = 10354.6394778\n",
      "Loss after 3300 iterations = 10514.7220132\n",
      "Loss after 3400 iterations = 10092.9320808\n",
      "Loss after 3500 iterations = 9811.13012772\n",
      "Loss after 3600 iterations = 10131.8399409\n",
      "Loss after 3700 iterations = 9874.968127\n",
      "Loss after 3800 iterations = 10386.8311795\n",
      "Loss after 3900 iterations = 10007.5327078\n",
      "Loss after 4000 iterations = 9998.91412128\n",
      "Loss after 4100 iterations = 9850.88812343\n",
      "Loss after 4200 iterations = 11792.0528159\n",
      "Loss after 4300 iterations = 10444.9255815\n",
      "Loss after 4400 iterations = 9634.93293601\n",
      "Loss after 4500 iterations = 9731.33433921\n",
      "Loss after 4600 iterations = 10002.2491777\n",
      "Loss after 4700 iterations = 11556.8963093\n",
      "Loss after 4800 iterations = 9760.7869684\n",
      "Loss after 4900 iterations = 10156.9162689\n",
      "Loss after 5000 iterations = 10362.24501\n",
      "Loss after 5100 iterations = 10045.3910321\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-6fa400b3ef25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtemp_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_jets_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjet_to_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_jets_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjet_to_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtemp_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtemp_te\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mte_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Masters/Machine learninig/Projects/ML_2017/project1/utils/implementations.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, lambda_, degree)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m#ws, loss = ridge_regression(y=train_y, lambda_=lambda_, tx=train_px)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_px\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m#ridge_term = (np.linalg.norm(ws, ord=2)) ** 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Masters/Machine learninig/Projects/ML_2017/project1/utils/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# converge criteria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Masters/Machine learninig/Projects/ML_2017/project1/utils/implementations.py\u001b[0m in \u001b[0;36mlearning_by_gradient_descent\u001b[0;34m(y, tx, w, gamma)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \"\"\"\n\u001b[1;32m     88\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss_neg_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Masters/Machine learninig/Projects/ML_2017/project1/utils/costfunction.py\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msigmoid\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0margument\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogaddexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lambda_ in lambdas:\n",
    "    temp_tr = np.zeros(k_fold)\n",
    "    temp_te = np.zeros(k_fold)\n",
    "    for k in range(k_fold):\n",
    "        tr_loss, te_loss, ws = cross_validation(y_jets_train[jet_to_train], x_jets_train[jet_to_train], k_indices, k, lambda_, degree)\n",
    "        temp_tr[k] = tr_loss\n",
    "        temp_te[k] = te_loss\n",
    "    print(np.mean(temp_te))\n",
    "    print(np.mean(temp_tr))\n",
    "    if np.mean(temp_te) < best_loss:\n",
    "        best_loss = np.mean(temp_te)\n",
    "        best_lambda = lambda_\n",
    "    print(\"After lambdas iteration, the best lambda is : \" + str(best_lambda) + \" for Lambda : \" + str(lambda_) + \" with best loss = \" + str(best_loss))\n",
    "    rmse_tr.append(np.mean(temp_tr))\n",
    "    rmse_te.append(np.mean(temp_te))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jet_to_train = 0\n",
    "degree = 1\n",
    "543.454079519\n",
    "3788.08242136\n",
    "\n",
    "jet_to_train = 0\n",
    "degree = 2\n",
    "527.606257202\n",
    "3660.47960373\n",
    "\n",
    "jet_to_train = 1\n",
    "degree = 1\n",
    "3945.51887954\n",
    "27574.96887\n",
    "\n",
    "jet_to_train = 1\n",
    "degree = 2\n",
    "3908.64952551\n",
    "27278.0120927\n",
    "\n",
    "jet_to_train = 1\n",
    "degree = 3\n",
    "BAD\n",
    "\n",
    "jet_to_train = 2\n",
    "degree = 1\n",
    "234.197314724 \n",
    "1630.67508124\n",
    "\n",
    "jet_to_train = 2\n",
    "degree = 2\n",
    "233.081735551\n",
    "1602.74241649\n",
    "\n",
    "jet_to_train = 2\n",
    "degree = 3\n",
    "236.43029354\n",
    "1564.26179544\n",
    "\n",
    "jet_to_train = 3\n",
    "degree = 1\n",
    "4085.45459208\n",
    "28568.0052354\n",
    "\n",
    "jet_to_train = 3\n",
    "degree = 2\n",
    "4009.65515713\n",
    "27994.504916\n",
    "\n",
    "jet_to_train = 4\n",
    "degree = 1\n",
    "108.914269772\n",
    "754.685716006\n",
    "\n",
    "jet_to_train = 4\n",
    "degree = 2\n",
    "103.775478397\n",
    "705.367011582\n",
    "\n",
    "jet_to_train = 4\n",
    "degree = 3\n",
    "109.135289693\n",
    "693.425231758\n",
    "\n",
    "jet_to_train = 5\n",
    "degree = 1\n",
    "2437.15647835\n",
    "17025.3416627\n",
    "\n",
    "jet_to_train = 5\n",
    "degree = 2\n",
    "2381.56121261\n",
    "16565.3445096\n",
    "\n",
    "jet_to_train = 5\n",
    "degree = 3\n",
    "2347.78598357\n",
    "16311.0713315\n",
    "\n",
    "jet_to_train = 6\n",
    "degree = 1\n",
    "45.6583051767\n",
    "314.975605086\n",
    "\n",
    "jet_to_train = 6\n",
    "degree = 2\n",
    "44.5916296727\n",
    "294.565592769\n",
    "\n",
    "jet_to_train = 7\n",
    "degree=1\n",
    "1164.10894967\n",
    "8119.8504021\n",
    "\n",
    "jet_to_train = 7\n",
    "degree= 2\n",
    "1118.08420289\n",
    "7722.93721149\n",
    "\n",
    "jet_to_train = 7\n",
    "degree = 3\n",
    "1094.32793334\n",
    "7572.61130825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how we obtain 83.10% accuracy, we should take a look on the model performance at various stages. The stages are divided into testing the raw data, the data divided into 4 categories each represeneting a specific jet number, and the data divided into 8 categories each represeting a jet number w/ and w/o the mass feature.\n",
    "The best results are obtained with 8 different classifiers in both models because now each classifier is trained on a specific subset of the whole dataset with a certain set of features that contributes specifically to that subset.\n",
    "Further investigation should be done to understand why the logistic regression is underperforming to the ridge regression model although we are dealing with a binary classifcation problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
