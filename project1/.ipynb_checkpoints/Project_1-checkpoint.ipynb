{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "**Finding the Higgs Boson**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libs and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train.csv'\n",
    "test_path = './data/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, ids = load_csv_data(train_path)\n",
    "ytest, testx, idstest = load_csv_data(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angle between arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle(vector1, vector2):\n",
    "    # cos(theta) = v1 dot v2 / ||v1|| * ||v2||\n",
    "    import numpy\n",
    "    numerator = numpy.dot(vector1, vector2)\n",
    "    denominator = numpy.linalg.norm(vector1) * numpy.linalg.norm(vector2)\n",
    "    x = numerator / denominator if denominator else 0\n",
    "    return numpy.arccos(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------problematics--------------------------------------\n",
    "# DERIVATIVE WITH PROBS\n",
    "prob_1 = 4\n",
    "prob_2 = 5\n",
    "prob_3 = 6\n",
    "prob_12 = 12\n",
    "# PRIMITIVE WITH PROBS\n",
    "pri_jet_pt = 23\n",
    "pri_jet_eta = 24\n",
    "pri_jet_phi = 25\n",
    "pri_sub_pt = 26\n",
    "pri_sub_eta = 27\n",
    "pri_sub_phi = 28\n",
    "pri_all_pt = 29\n",
    "\n",
    "# --------------------------------------good ones----------------------------------------\n",
    "# DERIVATIVE\n",
    "DER_mass_MMC = 0\n",
    "DER_mass_transverse_met_lep = 1\n",
    "DER_mass_vis = 2\n",
    "DER_pt_h = 3\n",
    "DER_deltar_tau_lep = 7\n",
    "DER_pt_tot = 8\n",
    "DER_sum_pt = 9\n",
    "DER_pt_ratio_lep_tau = 10\n",
    "DER_met_phi_centrality = 11\n",
    "met_phi = 20\n",
    "# PRIMITIVE\n",
    "pri_tau_pt = 13\n",
    "pri_tau_eta = 14\n",
    "pri_tau_phi = 15\n",
    "# ---------------\n",
    "pri_lep_pt = 16\n",
    "pri_lep_eta = 17\n",
    "pri_lep_phi = 18\n",
    "\n",
    "# Momentum vectors for TAU\n",
    "px_tau = x[:,pri_tau_pt]*np.sin(x[:,pri_tau_phi])\n",
    "py_tau = x[:,pri_tau_pt]*np.cos(x[:,pri_tau_phi])\n",
    "pz_tau = x[:,pri_tau_pt]*np.sinh(x[:,pri_tau_eta])\n",
    "mod_tau = x[:,pri_tau_pt]*np.cosh(x[:,pri_tau_eta])\n",
    "# Magnitude of TAU\n",
    "#magnitude_tau = np.sqrt(np.square(px_tau) + np.square(py_tau) + np.square(pz_tau))\n",
    "#magnitude_tau_square = (np.square(px_tau) + np.square(py_tau) + np.square(pz_tau))\n",
    "# Weinberg angle of TAU\n",
    "#wangle_tau = np.square(np.sin(x[:,pri_tau_phi]))\n",
    "\n",
    "# Momentum vectors for LEP\n",
    "px_lep = x[:,pri_lep_pt]*np.sin(x[:,pri_lep_phi])\n",
    "py_lep = x[:,pri_lep_pt]*np.cos(x[:,pri_lep_phi])\n",
    "pz_lep = x[:,pri_lep_pt]*np.sinh(x[:,pri_lep_eta])\n",
    "mod_lep = x[:,pri_lep_pt]*np.cosh(x[:,pri_lep_eta])\n",
    "# Magnitude of LEP\n",
    "#magnitude_lep = np.sqrt(np.square(px_lep) + np.square(py_lep) + np.square(pz_lep))\n",
    "#magnitude_lep_square = (np.square(px_lep) + np.square(py_lep) + np.square(pz_lep))\n",
    "# Weinberg angle of LEP\n",
    "#wangle_lep = np.square(np.sin(x[:,pri_lep_phi]))\n",
    "\n",
    "px_taut = testx[:,pri_tau_pt]*np.sin(testx[:,pri_tau_phi])\n",
    "py_taut = testx[:,pri_tau_pt]*np.cos(testx[:,pri_tau_phi])\n",
    "pz_taut = testx[:,pri_tau_pt]*np.sinh(testx[:,pri_tau_eta])\n",
    "mod_taut = testx[:,pri_tau_pt]*np.cosh(testx[:,pri_tau_eta])\n",
    "\n",
    "px_lept = testx[:,pri_lep_pt]*np.sin(testx[:,pri_lep_phi])\n",
    "py_lept = testx[:,pri_lep_pt]*np.cos(testx[:,pri_lep_phi])\n",
    "pz_lept = testx[:,pri_lep_pt]*np.sinh(testx[:,pri_lep_eta])\n",
    "mod_lept = testx[:,pri_lep_pt]*np.cosh(testx[:,pri_lep_eta])\n",
    "\n",
    "# Momentum vectors for JET PRIMARY\n",
    "# px_jet = x[:,pri_jet_pt]*np.sin(x[:,pri_jet_phi])\n",
    "# py_jet = x[:,pri_jet_pt]*np.cos(x[:,pri_jet_phi])\n",
    "# pz_jet = x[:,pri_jet_pt]*np.sinh(x[:,pri_jet_eta])\n",
    "# mod_jet = x[:,pri_jet_pt]*np.cosh(x[:,pri_jet_eta])\n",
    "\n",
    "# Momentum vectors for JET SECONDARY\n",
    "# px_sub = x[:,pri_sub_pt]*np.sin(x[:,pri_sub_phi])\n",
    "# py_sub = x[:,pri_sub_pt]*np.cos(x[:,pri_sub_phi])\n",
    "# pz_sub = x[:,pri_sub_pt]*np.sinh(x[:,pri_sub_eta])\n",
    "# mod_sub = x[:,pri_sub_pt]*np.cosh(x[:,pri_sub_eta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.column_stack((x, px_tau))\n",
    "x = np.column_stack((x, py_tau))\n",
    "x = np.column_stack((x, pz_tau))\n",
    "x = np.column_stack((x, mod_tau))\n",
    "x = np.column_stack((x, px_lep))\n",
    "x = np.column_stack((x, py_lep))\n",
    "x = np.column_stack((x, pz_lep))\n",
    "x = np.column_stack((x, mod_lep))\n",
    "\n",
    "testx = np.column_stack((testx, px_taut))\n",
    "testx = np.column_stack((testx, py_taut))\n",
    "testx = np.column_stack((testx, pz_taut))\n",
    "testx = np.column_stack((testx, mod_taut))\n",
    "testx = np.column_stack((testx, px_lept))\n",
    "testx = np.column_stack((testx, py_lept))\n",
    "testx = np.column_stack((testx, pz_lept))\n",
    "testx = np.column_stack((testx, mod_lept))\n",
    "testx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into different jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jets_0_train = x[x[:,22]==0]\n",
    "x_jets_1_train = x[x[:,22]==1]\n",
    "x_jets_2_train = x[x[:,22]==2]\n",
    "x_jets_3_train = x[x[:,22]==3]\n",
    "x_jets_train_matrix = np.array([x_jets_0_train,x_jets_1_train,x_jets_2_train,x_jets_3_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jets_0_test = testx[testx[:,22]==0]\n",
    "x_jets_1_test = testx[testx[:,22]==1]\n",
    "x_jets_2_test = testx[testx[:,22]==2]\n",
    "x_jets_3_test = testx[testx[:,22]==3]\n",
    "x_jets_test_matrix = np.array([x_jets_0_test,x_jets_1_test,x_jets_2_test,x_jets_3_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_x_0_train = np.where([x[:,22]==0])[1]\n",
    "indices_x_1_train = np.where([x[:,22]==1])[1]\n",
    "indices_x_2_train = np.where([x[:,22]==2])[1]\n",
    "indices_x_3_train = np.where([x[:,22]==3])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_x_0_test = np.where([testx[:,22]==0])[1]\n",
    "indices_x_1_test = np.where([testx[:,22]==1])[1]\n",
    "indices_x_2_test = np.where([testx[:,22]==2])[1]\n",
    "indices_x_3_test = np.where([testx[:,22]==3])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idstest_0 = idstest[indices_x_0_test]\n",
    "idstest_1 = idstest[indices_x_1_test]\n",
    "idstest_2 = idstest[indices_x_2_test]\n",
    "idstest_3 = idstest[indices_x_3_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idstest_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idstest_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idstest_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idstest_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_jets_0_train = y[indices_x_0_train]\n",
    "y_jets_1_train = y[indices_x_1_train]\n",
    "y_jets_2_train = y[indices_x_2_train]\n",
    "y_jets_3_train = y[indices_x_3_train]\n",
    "y_jets_train_matrix = np.array([y_jets_0_train,y_jets_1_train,y_jets_2_train,y_jets_3_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_jets_0_test = ytest[indices_x_0_test]\n",
    "y_jets_1_test = ytest[indices_x_1_test]\n",
    "y_jets_2_test = ytest[indices_x_2_test]\n",
    "y_jets_3_test = ytest[indices_x_3_test]\n",
    "y_jets_test_matrix = ([y_jets_0_test,y_jets_1_test,y_jets_2_test,y_jets_3_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "counter=collections.Counter(x[:,22])\n",
    "print(counter)\n",
    "counter = collections.Counter(testx[:,22])\n",
    "print(counter)\n",
    "for jets in y_jets_train_matrix:\n",
    "    counter = collections.Counter(jets)\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In jet = 0 train , there are 26123 Mass missing values from 99913. ~25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In jet = 1 train , there are 7562 Mass missing values from 77544. ~10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In jet = 2 train , there are 2952 Mass missing values from 50379. ~5.85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In jet = 3 train , there are 1477 Mass missing values from 22164. ~6.66%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests to make sure everything is alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape[0] == (x_jets_0_train.shape[0] + x_jets_1_train.shape[0] + x_jets_2_train.shape[0] + x_jets_3_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testx.shape[0] == (x_jets_0_test.shape[0] + x_jets_1_test.shape[0] + x_jets_2_test.shape[0] + x_jets_3_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape[0] == (y_jets_0_train.shape[0] + y_jets_1_train.shape[0] + y_jets_2_train.shape[0] + y_jets_3_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest.shape[0] == (y_jets_0_test.shape[0] + y_jets_1_test.shape[0] + y_jets_2_test.shape[0] + y_jets_3_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the un-needed indices from each jet, practically those are the indices that do not have any values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4,5,6,12,23,24,25,26,27,28,29 for jet 0\n",
    "#4,5,6,12,26,27,28\n",
    "x_delete_index_0 = [4,5,6,12,23,24,25,26,27,28,29]\n",
    "x_delete_index_1 = [4,5,6,12,26,27,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jets_0_train = np.delete(x_jets_0_train,x_delete_index_0,1)\n",
    "x_jets_0_test = np.delete(x_jets_0_test,x_delete_index_0,1)\n",
    "print(x_jets_0_train.shape)\n",
    "print(x_jets_0_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jets_1_train = np.delete(x_jets_1_train,x_delete_index_1,1)\n",
    "x_jets_1_test = np.delete(x_jets_1_test,x_delete_index_1,1)\n",
    "print(x_jets_1_train.shape)\n",
    "print(x_jets_1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_jets_2_train.shape)\n",
    "print(x_jets_2_test.shape)\n",
    "print(x_jets_3_train.shape)\n",
    "print(x_jets_3_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing the NaN values with the mean \n",
    "### Practically here we are only replacing the -999 in the first column with the average mean of each particular jet since it's the only column with mixed -999 values for all 4 jets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jet_matrix in x_jets_train_matrix:\n",
    "    jet_matrix[np.where(jet_matrix == -999)] = np.nan\n",
    "    me = np.ma.array(jet_matrix, mask=np.isnan(jet_matrix)).mean(axis=0)\n",
    "    means = np.ma.getdata(jet_matrix)\n",
    "    inds = np.where(np.isnan(jet_matrix))\n",
    "    jet_matrix[inds]=np.take(means,inds[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jet_matrix in x_jets_test_matrix:\n",
    "    jet_matrix[np.where(jet_matrix == -999)] = np.nan\n",
    "    me = np.ma.array(jet_matrix, mask=np.isnan(jet_matrix)).mean(axis=0)\n",
    "    means = np.ma.getdata(jet_matrix)\n",
    "    inds = np.where(np.isnan(jet_matrix))\n",
    "    jet_matrix[inds]=np.take(means,inds[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jets_0_train, x_jets_0_test = standardize(x_jets_0_train,x_jets_0_test)\n",
    "x_jets_1_train, x_jets_1_test = standardize(x_jets_1_train,x_jets_1_test)\n",
    "x_jets_2_train, x_jets_2_test = standardize(x_jets_2_train,x_jets_2_test)\n",
    "x_jets_3_train, x_jets_3_test = standardize(x_jets_3_train,x_jets_3_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create more features by building a polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 5\n",
    "\n",
    "px_train_0 = build_poly(degree=degree,x=x_jets_0_train)\n",
    "px_test_0 = build_poly(degree=degree,x=x_jets_0_test)\n",
    "px_train_1 = build_poly(degree=degree,x=x_jets_1_train)\n",
    "px_test_1 = build_poly(degree=degree,x=x_jets_1_test)\n",
    "px_train_2 = build_poly(degree=degree,x=x_jets_2_train)\n",
    "px_test_2 = build_poly(degree=degree,x=x_jets_2_test)\n",
    "px_train_3 = build_poly(degree=degree,x=x_jets_3_train)\n",
    "px_test_3 = build_poly(degree=degree,x=x_jets_3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(px_train_0.shape)\n",
    "print(px_train_1.shape)\n",
    "print(px_train_2.shape)\n",
    "print(px_train_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(px_test_0.shape)\n",
    "print(px_test_1.shape)\n",
    "print(px_test_2.shape)\n",
    "print(px_test_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_jets_0_train.shape)\n",
    "print(y_jets_1_train.shape)\n",
    "print(y_jets_2_train.shape)\n",
    "print(y_jets_3_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_0 = ridge_regression(lambda_=0.00067233575365, tx=px_train_0, y=y_jets_0_train)\n",
    "ws_1 = ridge_regression(lambda_=0.00067233575365, tx=px_train_1, y=y_jets_1_train)\n",
    "ws_2 = ridge_regression(lambda_=0.00067233575365, tx=px_train_2, y=y_jets_2_train)\n",
    "ws_3 = ridge_regression(lambda_=0.00067233575365, tx=px_train_3, y=y_jets_3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0 = predict_labels(ws_0, px_test_0)\n",
    "y_pred_1 = predict_labels(ws_1, px_test_1)\n",
    "y_pred_2 = predict_labels(ws_2, px_test_2)\n",
    "y_pred_3 = predict_labels(ws_3, px_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(idstest_0, y_pred_0, 'prediction_0.csv')\n",
    "create_csv_submission(idstest_1, y_pred_1, 'prediction_1.csv')\n",
    "create_csv_submission(idstest_2, y_pred_2, 'prediction_2.csv')\n",
    "create_csv_submission(idstest_3, y_pred_3, 'prediction_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_3.csv has been imported.\n",
      "prediction_2.csv has been imported.\n",
      "prediction_0.csv has been imported.\n",
      "prediction_1.csv has been imported.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import glob\n",
    "#import csv files from folder\n",
    "#path = r'data/US/market/merged_data'\n",
    "interesting_files = glob.glob(\"*.csv\") \n",
    "#allFiles = glob.glob(path + \"/*.csv\")\n",
    "with open('someoutputfile_1.csv', 'wb') as outfile:\n",
    "    for i, fname in enumerate(interesting_files):\n",
    "        with open(fname, 'rb') as infile:\n",
    "            if i != 0:\n",
    "                infile.readline()  # Throw away header on all but first file\n",
    "            # Block copy rest of file from input to output without parsing\n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "            print(fname + \" has been imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, csv ,operator\n",
    "data = csv.reader(open('someoutputfile_1.csv'),delimiter=',')\n",
    "sortedlist = sorted(data, key=operator.itemgetter(0))    # 0 specifies according to first column we want to sort\n",
    "      #now write the sorte result into new CSV file\n",
    "del sortedlist[len(sortedlist)-1]\n",
    "with open(\"Sorted_output.csv\", \"w\") as f:\n",
    "    fileWriter = csv.writer(f, delimiter=',')\n",
    "    fileWriter.writerow([\"Id\", \"Prediction\"])\n",
    "    for row in sortedlist:\n",
    "        fileWriter.writerow([int(row[0]), int(row[1])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_0, train_y_0, test_x_0, test_y_0 = split_data(px_train_0, y_jets_0_train, 0.8, seed=1)\n",
    "train_x_1, train_y_1, test_x_1, test_y_1 = split_data(px_train_1, y_jets_1_train, 0.8, seed=1)\n",
    "train_x_2, train_y_2, test_x_2, test_y_2 = split_data(px_train_2, y_jets_2_train, 0.8, seed=1)\n",
    "train_x_3, train_y_3, test_x_3, test_y_3 = split_data(px_train_3, y_jets_3_train, 0.8, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_0 = ridge_regression(lambda_=0.00067233575365, tx=train_x_0, y=train_y_0)\n",
    "ws_1 = ridge_regression(lambda_=0.00067233575365, tx=train_x_1, y=train_y_1)\n",
    "ws_2 = ridge_regression(lambda_=0.00067233575365, tx=train_x_2, y=train_y_2)\n",
    "ws_3 = ridge_regression(lambda_=0.00067233575365, tx=train_x_3, y=train_y_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0 = predict_labels(ws_0, px_test_0)\n",
    "y_pred_1 = predict_labels(ws_1, px_test_1)\n",
    "y_pred_2 = predict_labels(ws_2, px_test_2)\n",
    "y_pred_3 = predict_labels(ws_3, px_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_0 = 1 - np.mean( y_pred_0 != test_y_0 )\n",
    "print(\"Accuracy: \" + str(accuracy_0) + \"%\")\n",
    "accuracy_1 = 1 - np.mean( y_pred_1 != test_y_1 )\n",
    "print(\"Accuracy: \" + str(accuracy_1) + \"%\")\n",
    "accuracy_2 = 1 - np.mean( y_pred_2 != test_y_2 )\n",
    "print(\"Accuracy: \" + str(accuracy_2) + \"%\")\n",
    "accuracy_3 = 1 - np.mean( y_pred_3 != test_y_3 )\n",
    "print(\"Accuracy: \" + str(accuracy_3) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Added Features:\n",
    "    1-Accuracy: 0.720762648251%\n",
    "    2-Accuracy: 0.684118898704%\n",
    "    3-Accuracy: 0.740075426757%\n",
    "    4-Accuracy: 0.661403113016%\n",
    "### Polynomial 3 + added features:\n",
    "    1-Accuracy: 0.834709503078%\n",
    "    2-Accuracy: 0.761299890386%\n",
    "    3-Accuracy: 0.793370385073%\n",
    "    4-Accuracy: 0.798330701557%\n",
    "### Polynomial 5 + added features:\n",
    "    1-Accuracy: 0.843316819296%\n",
    "    2-Accuracy: 0.780965890773%\n",
    "    3-Accuracy: 0.804485907106%\n",
    "    4-Accuracy: 0.810286487706%\n",
    "### Polynomial 7 + added features:\n",
    "    1-Accuracy: 0.84561877596%\n",
    "    2-Accuracy: 0.783609517055%\n",
    "    3-Accuracy: 0.815204446209%\n",
    "    4-Accuracy: 0.818407399053%\n",
    "### Polynomial 9+ added features:\n",
    "    1-Accuracy: 0.848170945304%\n",
    "    2-Accuracy: 0.800309497711%\n",
    "    3-Accuracy: 0.82423580786%\n",
    "    4-Accuracy: 0.838258515678%\n",
    "### Polynomial 11+ added features:\n",
    "    1-Accuracy: 0.849371966171%\n",
    "    2-Accuracy: 0.799535753433%\n",
    "    3-Accuracy: 0.824930527987%\n",
    "    4-Accuracy: 0.837356192195%\n",
    "### Polynomial 13+ added featuers:\n",
    "    1-Accuracy: 0.511034379222%\n",
    "    2-Accuracy: 0.802050422335%\n",
    "    3-Accuracy: 0.771734815403%\n",
    "    4-Accuracy: 0.82066320776%\n",
    "### Polynomial 15 + added features:\n",
    "    1-Accuracy: 0.555922534154%\n",
    "    2-Accuracy: 0.773099490618%\n",
    "    3-Accuracy: 0.636661373561%\n",
    "    4-Accuracy: 0.663884502594%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing column features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_feature_to_delete = [pri_jet_phi,pri_sub_phi,pri_tau_phi,pri_lep_phi,met_phi]\n",
    "x = np.delete(x, columns_feature_to_delete, 1)\n",
    "testx = np.delete(testx, columns_feature_to_delete, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x[np.where(x == -999)] = np.nan\n",
    "me = np.ma.array(x, mask=np.isnan(x)).mean(axis=0)\n",
    "means = np.ma.getdata(me)\n",
    "inds = np.where(np.isnan(x))\n",
    "x[inds]=np.take(means,inds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testx[np.where(testx == -999)] = np.nan\n",
    "me = np.ma.array(testx, mask=np.isnan(testx)).mean(axis=0)\n",
    "means = np.ma.getdata(me)\n",
    "inds = np.where(np.isnan(testx))\n",
    "testx[inds]=np.take(means,inds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(x)\n",
    "testx, mean_x, std_x = standardize(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[2]\n",
    "testx[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = split_data(x, y, 0.8, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 6\n",
    "px_train = build_poly(degree=degree,x=train_x)\n",
    "px_test = build_poly(degree=degree,x=test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "degree = 6\n",
    "px_train = build_poly(degree=degree,x=x)\n",
    "px_test = build_poly(degree=degree,x=testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.0000001\n",
    "    losses = []\n",
    "\n",
    "    y[y == -1] = 0\n",
    "    # build tx\n",
    "    w = np.zeros((x.shape[1], ))\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, x, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, x, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logistic_regression_gradient_descent_demo(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = ridge_regression(lambda_=0.00067233575365, tx=px_train, y=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = compute_mse(train_y, px_train, ws)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = compute_mse(ytest, px_test, ws)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(ws, px_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 1 - np.mean( y_pred != test_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" + str(accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(idstest, y_pred, 'prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degree = 6\n",
    "k_fold = 10\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "best_loss = 999\n",
    "for k in range(k_fold):\n",
    "    temp_tr = []\n",
    "    temp_te = []\n",
    "    best_test_ws = []\n",
    "    for lambda_ in lambdas:\n",
    "        tr_loss, te_loss, ws = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "        if(te_loss < best_loss):\n",
    "            best_loss = te_loss\n",
    "            best_lambda = lambda_\n",
    "        temp_tr.append(tr_loss)\n",
    "        temp_te.append(te_loss)\n",
    "        # print(\"Lambda = \" + str(lambda_) + \" tr_loss = \" + str(tr_loss) + \" te_loss = \" + str(te_loss))\n",
    "    print(\"After lambdas iteration, the best lambda is : \" + str(best_lambda) + \" for k-fold : \" + str(k) + \" with best loss = \" + str(best_loss))\n",
    "    best_test_ws.append(lambda_)\n",
    "    rmse_tr.append(temp_tr)\n",
    "    rmse_te.append(temp_te)\n",
    "\n",
    "rmse_tr = np.matrix(rmse_tr)\n",
    "rmse_tr = np.mean(rmse_tr, axis=0)\n",
    "rmse_tr = np.reshape(rmse_tr, (len(lambdas),-1))\n",
    "rmse_te = np.matrix(rmse_te)\n",
    "rmse_te = np.mean(rmse_te, axis=0)\n",
    "rmse_te = np.reshape(rmse_te, (len(lambdas),-1))\n",
    "\n",
    "cross_validation_visualization(lambdas, rmse_tr, rmse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # return loss, gradient and hessian: TODO\n",
    "    # ***************************************************\n",
    "    print('here it is')\n",
    "    loss, gradient, hessian = logistic_regression(y,tx,w)\n",
    "    \n",
    "    #raise NotImplementedError\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # update w: TODO\n",
    "    # ***************************************************\n",
    "    print('here it is')\n",
    "    termine = np.linalg.solve(hessian,gradient)\n",
    "    w = w-termine\n",
    "    \n",
    "    #raise NotImplementedError\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.01\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((x.shape[1], ))\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, x, w)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_newton_method\")\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic_regression_newton_method_demo(y,x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
