{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "**Finding the Higgs Boson**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libs and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = './data/train.csv'\n",
    "test_path = './data/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y, x, ids = load_csv_data(train_path)\n",
    "ytest, xtest, idstest = load_csv_data(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DERIVATIVE\n",
    "DER_mass_MMC = 0\n",
    "DER_mass_transverse_met_lep = 1\n",
    "DER_mass_vis = 2\n",
    "DER_pt_h = 3\n",
    "DER_deltar_tau_lep = 7\n",
    "DER_pt_tot = 8\n",
    "DER_sum_pt = 9\n",
    "DER_pt_ratio_lep_tau = 10\n",
    "DER_met_phi_centrality = 11\n",
    "met_phi = 20\n",
    "\n",
    "# PRIMITIVE\n",
    "pri_tau_pt = 13\n",
    "pri_tau_eta = 14\n",
    "pri_tau_phi = 15\n",
    "# ---------------\n",
    "pri_lep_pt = 16\n",
    "pri_lep_eta = 17\n",
    "pri_lep_phi = 18\n",
    "\n",
    "# ------------------------------------problematics--------------------------------------\n",
    "# DERIVATIVE WITH PROBS\n",
    "prob_1 = 4\n",
    "prob_2 = 5\n",
    "prob_3 = 6\n",
    "prob_12 = 12\n",
    "# PRIMITIVE WITH PROBS\n",
    "pri_jet_pt = 23\n",
    "pri_jet_eta = 24\n",
    "pri_jet_phi = 25\n",
    "pri_sub_pt = 26\n",
    "pri_sub_eta = 27\n",
    "pri_sub_phi = 28\n",
    "pri_all_pt = 29\n",
    "\n",
    "# Momentum vectors for TAU\n",
    "px_tau = x[:,pri_tau_pt]*np.sin(x[:,pri_tau_phi])\n",
    "py_tau = x[:,pri_tau_pt]*np.cos(x[:,pri_tau_phi])\n",
    "pz_tau = x[:,pri_tau_pt]*np.sinh(x[:,pri_tau_eta])\n",
    "mod_tau = x[:,pri_tau_pt]*np.cosh(x[:,pri_tau_eta])\n",
    "\n",
    "# Momentum vectors for LEP\n",
    "px_lep = x[:,pri_lep_pt]*np.sin(x[:,pri_lep_phi])\n",
    "py_lep = x[:,pri_lep_pt]*np.cos(x[:,pri_lep_phi])\n",
    "pz_lep = x[:,pri_lep_pt]*np.sinh(x[:,pri_lep_eta])\n",
    "mod_lep = x[:,pri_lep_pt]*np.cosh(x[:,pri_lep_eta])\n",
    "\n",
    "# Momentum vectors for JET PRIMARY\n",
    "# px_jet = x[:,pri_jet_pt]*np.sin(x[:,pri_jet_phi])\n",
    "# py_jet = x[:,pri_jet_pt]*np.cos(x[:,pri_jet_phi])\n",
    "# pz_jet = x[:,pri_jet_pt]*np.sinh(x[:,pri_jet_eta])\n",
    "# mod_jet = x[:,pri_jet_pt]*np.cosh(x[:,pri_jet_eta])\n",
    "\n",
    "# Momentum vectors for JET SECONDARY\n",
    "# px_sub = x[:,pri_sub_pt]*np.sin(x[:,pri_sub_phi])\n",
    "# py_sub = x[:,pri_sub_pt]*np.cos(x[:,pri_sub_phi])\n",
    "# pz_sub = x[:,pri_sub_pt]*np.sinh(x[:,pri_sub_eta])\n",
    "# mod_sub = x[:,pri_sub_pt]*np.cosh(x[:,pri_sub_eta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 38)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.column_stack((x, px_tau))\n",
    "x = np.column_stack((x, py_tau))\n",
    "x = np.column_stack((x, pz_tau))\n",
    "x = np.column_stack((x, mod_tau))\n",
    "x = np.column_stack((x, px_lep))\n",
    "x = np.column_stack((x, py_lep))\n",
    "x = np.column_stack((x, pz_lep))\n",
    "x = np.column_stack((x, mod_lep))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing column features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_feature_to_delete = [pri_jet_phi,pri_sub_phi,pri_tau_phi,pri_lep_phi,met_phi]\n",
    "x = np.delete(x, columns_feature_to_delete, 1)\n",
    "xtest = np.delete(xtest, columns_feature_to_delete, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 33)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x[np.where(x == -999)] = np.nan\n",
    "me = np.ma.array(x, mask=np.isnan(x)).mean(axis=0)\n",
    "means = np.ma.getdata(me)\n",
    "inds = np.where(np.isnan(x))\n",
    "x[inds]=np.take(means,inds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.21858528e+02,   1.62172000e+02,   1.25953000e+02,\n",
       "         3.56350000e+01,   2.40373503e+00,   3.71783360e+02,\n",
       "        -8.21688171e-01,   3.14800000e+00,   9.33600000e+00,\n",
       "         1.97814000e+02,   3.77600000e+00,   1.41400000e+00,\n",
       "         4.58289801e-01,   3.21540000e+01,  -7.05000000e-01,\n",
       "         1.21409000e+02,  -9.53000000e-01,   5.42830000e+01,\n",
       "         2.60414000e+02,   1.00000000e+00,   4.42510000e+01,\n",
       "         2.05300000e+00,   5.76794744e+01,  -1.18452642e-02,\n",
       "         4.42510000e+01,  -2.78685828e+01,  -1.60381361e+01,\n",
       "        -2.45935996e+01,   4.04811667e+01,   1.05433595e+02,\n",
       "         6.01988567e+01,  -1.34029216e+02,   1.80842407e+02])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.31421984e-16   4.19362323e-17  -3.43618467e-17   4.41104930e-17\n",
      "   3.60216745e-16   2.01623607e-16  -5.11306553e-17  -5.14546628e-16\n",
      "  -4.54633664e-16  -1.03227649e-16   3.73262310e-16  -3.55555585e-17\n",
      "  -1.05956133e-16   4.05350420e-16   1.33013600e-17  -5.94468474e-16\n",
      "   1.35571554e-17  -3.88581611e-16  -4.52388349e-16   1.11981535e-17\n",
      "   2.63526090e-16  -3.41060513e-19   3.53566065e-17  -1.84741111e-19\n",
      "  -2.37520226e-16   1.77635684e-17  -1.03455022e-17  -2.11173301e-17\n",
      "   9.23705556e-17   1.75646164e-17   7.61701813e-18   4.66116035e-18\n",
      "   9.06652531e-18]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "[  4.56407524e-18  -2.45834847e-16  -6.11542317e-16   5.75198523e-19\n",
      "   6.83923548e-16  -1.71434169e-17  -8.75402135e-16   4.30586112e-16\n",
      "   2.78683685e-16   2.46410046e-16   8.07028537e-17   5.32746371e-17\n",
      "  -2.76482925e-16   2.67317262e-16  -7.34003333e-18   5.74323221e-17\n",
      "   2.59902202e-17  -1.90315685e-17   7.74642359e-16  -2.50836573e-17\n",
      "  -1.01885165e-16  -5.79199904e-17   2.35124901e-16  -8.30424111e-16\n",
      "  -4.19907426e-16]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "x, mean_x, std_x = standardize(x)\n",
    "testx, mean_x, std_x = standardize(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   3.19515553e+00,   1.09655998e+00,\n",
       "        -3.49709651e-01,   4.73195814e-16,   0.00000000e+00,\n",
       "        -5.75007627e-17,   9.89769704e-01,  -4.30168330e-01,\n",
       "         3.40361109e-01,   2.76817375e+00,   1.29216437e+00,\n",
       "         0.00000000e+00,  -2.92406240e-01,  -5.71650232e-01,\n",
       "         3.38768208e+00,  -7.37950650e-01,   3.82000541e-01,\n",
       "         4.00135347e-01,   2.13049736e-02,  -8.63173389e-01,\n",
       "         1.48714489e+00,   0.00000000e+00,  -1.74353029e-17,\n",
       "        -2.93969845e-01,  -8.72991448e-01,  -5.03547399e-01,\n",
       "        -2.89035318e-01,  -5.65014819e-01,   2.88750489e+00,\n",
       "         1.63516804e+00,  -1.24764903e+00,   1.21492897e+00])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = np.ma.getdata(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xtest = np.ma.getdata(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# me = np.ma.array(x, mask=np.isnan(x)).mean(axis=0)\n",
    "# means = np.ma.getdata(me)\n",
    "# means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x = np.nan_to_num(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inds = np.where(np.isnan(x))\n",
    "# x[inds]=np.take(means,inds[1])\n",
    "# x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = split_data(x, y, 0.8, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degree = 6\n",
    "px_train = build_poly(degree=degree,x=train_x)\n",
    "px_test = build_poly(degree=degree,x=test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 199)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "degree = 10\n",
    "px_train = build_poly(degree=degree,x=x)\n",
    "px_test = build_poly(degree=degree,x=xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.0000001\n",
    "    losses = []\n",
    "\n",
    "    y[y == -1] = 0\n",
    "    # build tx\n",
    "    w = np.zeros((x.shape[1], ))\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, x, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, x, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=173286.79513998653\n",
      "Current iteration=1, loss=172502.18470433177\n",
      "Current iteration=2, loss=171753.12930779433\n",
      "Current iteration=3, loss=171037.56370953232\n",
      "Current iteration=4, loss=170353.5394665347\n",
      "Current iteration=5, loss=169699.2225117463\n",
      "Current iteration=6, loss=169072.889762671\n",
      "Current iteration=7, loss=168472.92498931434\n",
      "Current iteration=8, loss=167897.81414872222\n",
      "Current iteration=9, loss=167346.14036730325\n",
      "Current iteration=10, loss=166816.57872428128\n",
      "Current iteration=11, loss=166307.8909621195\n",
      "Current iteration=12, loss=165818.92022407483\n",
      "Current iteration=13, loss=165348.5858960702\n",
      "Current iteration=14, loss=164895.87861023037\n",
      "Current iteration=15, loss=164459.8554507745\n",
      "Current iteration=16, loss=164039.63538933528\n",
      "Current iteration=17, loss=163634.3949658796\n",
      "Current iteration=18, loss=163243.3642228866\n",
      "Current iteration=19, loss=162865.8228939259\n",
      "Current iteration=20, loss=162501.09684292568\n",
      "Current iteration=21, loss=162148.5547469091\n",
      "Current iteration=22, loss=161807.6050125413\n",
      "Current iteration=23, loss=161477.69291522843\n",
      "Current iteration=24, loss=161158.29794856207\n",
      "Current iteration=25, loss=160848.93137144423\n",
      "Current iteration=26, loss=160549.1339401381\n",
      "Current iteration=27, loss=160258.47381266832\n",
      "Current iteration=28, loss=159976.54461336066\n",
      "Current iteration=29, loss=159702.96364580945\n",
      "Current iteration=30, loss=159437.37024314038\n",
      "Current iteration=31, loss=159179.42424506586\n",
      "Current iteration=32, loss=158928.80459187954\n",
      "Current iteration=33, loss=158685.2080261919\n",
      "Current iteration=34, loss=158448.3478938511\n",
      "Current iteration=35, loss=158217.95303611748\n",
      "Current iteration=36, loss=157993.76676575615\n",
      "Current iteration=37, loss=157775.54592027917\n",
      "Current iteration=38, loss=157563.05998610266\n",
      "Current iteration=39, loss=157356.0902878835\n",
      "Current iteration=40, loss=157154.42923776765\n",
      "Current iteration=41, loss=156957.8796397143\n",
      "Current iteration=42, loss=156766.25404446208\n",
      "Current iteration=43, loss=156579.3741510736\n",
      "Current iteration=44, loss=156397.07025133484\n",
      "Current iteration=45, loss=156219.18071360159\n",
      "Current iteration=46, loss=156045.55150297104\n",
      "Current iteration=47, loss=155876.03573492132\n",
      "Current iteration=48, loss=155710.49325980298\n",
      "Current iteration=49, loss=155548.79027578715\n",
      "Current iteration=50, loss=155390.79896807624\n",
      "Current iteration=51, loss=155236.39717236787\n",
      "Current iteration=52, loss=155085.4680607294\n",
      "Current iteration=53, loss=154937.89984819476\n",
      "Current iteration=54, loss=154793.58551853357\n",
      "Current iteration=55, loss=154652.4225677714\n",
      "Current iteration=56, loss=154514.3127641544\n",
      "Current iteration=57, loss=154379.16192335897\n",
      "Current iteration=58, loss=154246.8796978427\n",
      "Current iteration=59, loss=154117.37937932176\n",
      "Current iteration=60, loss=153990.57771344006\n",
      "Current iteration=61, loss=153866.3947257694\n",
      "Current iteration=62, loss=153744.75355834715\n",
      "Current iteration=63, loss=153625.58031601916\n",
      "Current iteration=64, loss=153508.80392191318\n",
      "Current iteration=65, loss=153394.35598141805\n",
      "Current iteration=66, loss=153282.17065409292\n",
      "Current iteration=67, loss=153172.184532973\n",
      "Current iteration=68, loss=153064.33653077914\n",
      "Current iteration=69, loss=152958.5677725744\n",
      "Current iteration=70, loss=152854.82149444486\n",
      "Current iteration=71, loss=152753.04294781297\n",
      "Current iteration=72, loss=152653.17930902\n",
      "Current iteration=73, loss=152555.17959384\n",
      "Current iteration=74, loss=152458.9945766131\n",
      "Current iteration=75, loss=152364.57671370648\n",
      "Current iteration=76, loss=152271.88007103367\n",
      "Current iteration=77, loss=152180.8602553802\n",
      "Current iteration=78, loss=152091.47434930247\n",
      "Current iteration=79, loss=152003.68084938178\n",
      "Current iteration=80, loss=151917.43960763115\n",
      "Current iteration=81, loss=151832.71177586608\n",
      "Current iteration=82, loss=151749.45975286295\n",
      "Current iteration=83, loss=151667.6471341414\n",
      "Current iteration=84, loss=151587.23866421657\n",
      "Current iteration=85, loss=151508.2001911791\n",
      "Current iteration=86, loss=151430.49862346821\n",
      "Current iteration=87, loss=151354.1018887139\n",
      "Current iteration=88, loss=151278.9788945302\n",
      "Current iteration=89, loss=151205.0994911515\n",
      "Current iteration=90, loss=151132.43443580822\n",
      "Current iteration=91, loss=151060.9553587472\n",
      "Current iteration=92, loss=150990.63473080602\n",
      "Current iteration=93, loss=150921.4458324575\n",
      "Current iteration=94, loss=150853.36272424544\n",
      "Current iteration=95, loss=150786.36021853707\n",
      "Current iteration=96, loss=150720.41385252302\n",
      "Current iteration=97, loss=150655.4998623993\n",
      "Current iteration=98, loss=150591.59515866975\n",
      "Current iteration=99, loss=150528.67730251123\n",
      "Current iteration=100, loss=150466.7244831475\n",
      "Current iteration=101, loss=150405.71549618\n",
      "Current iteration=102, loss=150345.6297228282\n",
      "Current iteration=103, loss=150286.4471100335\n",
      "Current iteration=104, loss=150228.1481513843\n",
      "Current iteration=105, loss=150170.71386882174\n",
      "Current iteration=106, loss=150114.12579508848\n",
      "Current iteration=107, loss=150058.36595688388\n",
      "Current iteration=108, loss=150003.41685869274\n",
      "Current iteration=109, loss=149949.2614672546\n",
      "Current iteration=110, loss=149895.88319664428\n",
      "Current iteration=111, loss=149843.26589393432\n",
      "Current iteration=112, loss=149791.39382541284\n",
      "Current iteration=113, loss=149740.25166333103\n",
      "Current iteration=114, loss=149689.82447315598\n",
      "Current iteration=115, loss=149640.09770130625\n",
      "Current iteration=116, loss=149591.05716334813\n",
      "Current iteration=117, loss=149542.6890326322\n",
      "Current iteration=118, loss=149494.97982935057\n",
      "Current iteration=119, loss=149447.91640999675\n",
      "Current iteration=120, loss=149401.48595720957\n",
      "Current iteration=121, loss=149355.6759699859\n",
      "Current iteration=122, loss=149310.47425424488\n",
      "Current iteration=123, loss=149265.86891372997\n",
      "Current iteration=124, loss=149221.84834123348\n",
      "Current iteration=125, loss=149178.4012101306\n",
      "Current iteration=126, loss=149135.51646620984\n",
      "Current iteration=127, loss=149093.18331978767\n",
      "Current iteration=128, loss=149051.39123809538\n",
      "Current iteration=129, loss=149010.12993792756\n",
      "Current iteration=130, loss=148969.389378541\n",
      "Current iteration=131, loss=148929.1597547944\n",
      "Current iteration=132, loss=148889.43149051894\n",
      "Current iteration=133, loss=148850.1952321106\n",
      "Current iteration=134, loss=148811.4418423358\n",
      "Current iteration=135, loss=148773.16239434137\n",
      "Current iteration=136, loss=148735.3481658615\n",
      "Current iteration=137, loss=148697.99063361363\n",
      "Current iteration=138, loss=148661.08146787636\n",
      "Current iteration=139, loss=148624.61252724208\n",
      "Current iteration=140, loss=148588.57585353806\n",
      "Current iteration=141, loss=148552.96366690923\n",
      "Current iteration=142, loss=148517.7683610571\n",
      "Current iteration=143, loss=148482.98249862852\n",
      "Current iteration=144, loss=148448.59880674904\n",
      "Current iteration=145, loss=148414.6101726955\n",
      "Current iteration=146, loss=148381.0096397028\n",
      "Current iteration=147, loss=148347.7904028996\n",
      "Current iteration=148, loss=148314.9458053692\n",
      "Current iteration=149, loss=148282.4693343299\n",
      "Current iteration=150, loss=148250.3546174316\n",
      "Current iteration=151, loss=148218.5954191642\n",
      "Current iteration=152, loss=148187.18563737336\n",
      "Current iteration=153, loss=148156.11929988078\n",
      "Current iteration=154, loss=148125.39056120455\n",
      "Current iteration=155, loss=148094.99369937656\n",
      "Current iteration=156, loss=148064.92311285358\n",
      "Current iteration=157, loss=148035.17331751846\n",
      "Current iteration=158, loss=148005.73894376893\n",
      "Current iteration=159, loss=147976.61473369057\n",
      "Current iteration=160, loss=147947.79553831133\n",
      "Current iteration=161, loss=147919.2763149349\n",
      "Current iteration=162, loss=147891.05212455013\n",
      "Current iteration=163, loss=147863.1181293141\n",
      "Current iteration=164, loss=147835.4695901065\n",
      "Current iteration=165, loss=147808.10186415285\n",
      "Current iteration=166, loss=147781.01040271402\n",
      "Current iteration=167, loss=147754.1907488409\n",
      "Current iteration=168, loss=147727.63853519064\n",
      "Current iteration=169, loss=147701.34948190395\n",
      "Current iteration=170, loss=147675.31939454042\n",
      "Current iteration=171, loss=147649.5441620706\n",
      "Current iteration=172, loss=147624.01975492286\n",
      "Current iteration=173, loss=147598.7422230832\n",
      "Current iteration=174, loss=147573.70769424664\n",
      "Current iteration=175, loss=147548.91237201833\n",
      "Current iteration=176, loss=147524.352534163\n",
      "Current iteration=177, loss=147500.02453090082\n",
      "Current iteration=178, loss=147475.9247832492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=179, loss=147452.049781408\n",
      "Current iteration=180, loss=147428.39608318746\n",
      "Current iteration=181, loss=147404.9603124777\n",
      "Current iteration=182, loss=147381.73915775787\n",
      "Current iteration=183, loss=147358.72937064458\n",
      "Current iteration=184, loss=147335.92776447764\n",
      "Current iteration=185, loss=147313.33121294255\n",
      "Current iteration=186, loss=147290.9366487286\n",
      "Current iteration=187, loss=147268.74106222103\n",
      "Current iteration=188, loss=147246.7415002271\n",
      "Current iteration=189, loss=147224.9350647342\n",
      "Current iteration=190, loss=147203.31891169958\n",
      "Current iteration=191, loss=147181.89024987083\n",
      "Current iteration=192, loss=147160.6463396358\n",
      "Current iteration=193, loss=147139.5844919015\n",
      "Current iteration=194, loss=147118.70206700105\n",
      "Current iteration=195, loss=147097.9964736278\n",
      "Current iteration=196, loss=147077.46516779574\n",
      "Current iteration=197, loss=147057.10565182596\n",
      "Current iteration=198, loss=147036.9154733577\n",
      "Current iteration=199, loss=147016.89222438392\n",
      "Current iteration=200, loss=146997.0335403103\n",
      "Current iteration=201, loss=146977.3370990373\n",
      "Current iteration=202, loss=146957.8006200644\n",
      "Current iteration=203, loss=146938.42186361607\n",
      "Current iteration=204, loss=146919.19862978882\n",
      "Current iteration=205, loss=146900.12875771866\n",
      "Current iteration=206, loss=146881.21012476872\n",
      "Current iteration=207, loss=146862.440645736\n",
      "Current iteration=208, loss=146843.81827207704\n",
      "Current iteration=209, loss=146825.34099115225\n",
      "Current iteration=210, loss=146807.00682548754\n",
      "Current iteration=211, loss=146788.81383205394\n",
      "Current iteration=212, loss=146770.7601015637\n",
      "Current iteration=213, loss=146752.84375778298\n",
      "Current iteration=214, loss=146735.06295686067\n",
      "Current iteration=215, loss=146717.4158866727\n",
      "Current iteration=216, loss=146699.90076618167\n",
      "Current iteration=217, loss=146682.515844811\n",
      "Current iteration=218, loss=146665.25940183375\n",
      "Current iteration=219, loss=146648.1297457755\n",
      "Current iteration=220, loss=146631.12521383067\n",
      "Current iteration=221, loss=146614.24417129223\n",
      "Current iteration=222, loss=146597.48501099448\n",
      "Current iteration=223, loss=146580.84615276832\n",
      "Current iteration=224, loss=146564.32604290906\n",
      "Current iteration=225, loss=146547.9231536558\n",
      "Current iteration=226, loss=146531.63598268304\n",
      "Current iteration=227, loss=146515.46305260315\n",
      "Current iteration=228, loss=146499.40291048033\n",
      "Current iteration=229, loss=146483.4541273553\n",
      "Current iteration=230, loss=146467.6152977803\n",
      "Current iteration=231, loss=146451.8850393648\n",
      "Current iteration=232, loss=146436.26199233078\n",
      "Current iteration=233, loss=146420.7448190782\n",
      "Current iteration=234, loss=146405.3322037597\n",
      "Current iteration=235, loss=146390.02285186475\n",
      "Current iteration=236, loss=146374.81548981287\n",
      "Current iteration=237, loss=146359.7088645556\n",
      "Current iteration=238, loss=146344.7017431872\n",
      "Current iteration=239, loss=146329.7929125637\n",
      "Current iteration=240, loss=146314.9811789302\n",
      "Current iteration=241, loss=146300.26536755625\n",
      "Current iteration=242, loss=146285.64432237876\n",
      "Current iteration=243, loss=146271.11690565306\n",
      "Current iteration=244, loss=146256.68199761078\n",
      "Current iteration=245, loss=146242.33849612557\n",
      "Current iteration=246, loss=146228.08531638546\n",
      "Current iteration=247, loss=146213.92139057256\n",
      "Current iteration=248, loss=146199.845667549\n",
      "Current iteration=249, loss=146185.85711255015\n",
      "Current iteration=250, loss=146171.95470688347\n",
      "Current iteration=251, loss=146158.1374476345\n",
      "Current iteration=252, loss=146144.40434737827\n",
      "Current iteration=253, loss=146130.75443389712\n",
      "Current iteration=254, loss=146117.18674990436\n",
      "Current iteration=255, loss=146103.7003527734\n",
      "Current iteration=256, loss=146090.29431427267\n",
      "Current iteration=257, loss=146076.96772030607\n",
      "Current iteration=258, loss=146063.71967065852\n",
      "Current iteration=259, loss=146050.5492787469\n",
      "Current iteration=260, loss=146037.45567137594\n",
      "Current iteration=261, loss=146024.43798849927\n",
      "Current iteration=262, loss=146011.49538298513\n",
      "Current iteration=263, loss=145998.627020387\n",
      "Current iteration=264, loss=145985.83207871867\n",
      "Current iteration=265, loss=145973.10974823404\n",
      "Current iteration=266, loss=145960.4592312113\n",
      "Current iteration=267, loss=145947.87974174137\n",
      "Current iteration=268, loss=145935.37050552058\n",
      "Current iteration=269, loss=145922.93075964763\n",
      "Current iteration=270, loss=145910.55975242436\n",
      "Current iteration=271, loss=145898.25674316077\n",
      "Current iteration=272, loss=145886.0210019836\n",
      "Current iteration=273, loss=145873.8518096489\n",
      "Current iteration=274, loss=145861.74845735836\n",
      "Current iteration=275, loss=145849.71024657882\n",
      "Current iteration=276, loss=145837.736488866\n",
      "Current iteration=277, loss=145825.82650569107\n",
      "Current iteration=278, loss=145813.97962827096\n",
      "Current iteration=279, loss=145802.1951974018\n",
      "Current iteration=280, loss=145790.47256329577\n",
      "Current iteration=281, loss=145778.81108542092\n",
      "Current iteration=282, loss=145767.21013234425\n",
      "Current iteration=283, loss=145755.6690815777\n",
      "Current iteration=284, loss=145744.1873194272\n",
      "Current iteration=285, loss=145732.76424084458\n",
      "Current iteration=286, loss=145721.39924928223\n",
      "Current iteration=287, loss=145710.0917565508\n",
      "Current iteration=288, loss=145698.84118267926\n",
      "Current iteration=289, loss=145687.6469557779\n",
      "Current iteration=290, loss=145676.50851190384\n",
      "Current iteration=291, loss=145665.4252949291\n",
      "Current iteration=292, loss=145654.39675641104\n",
      "Current iteration=293, loss=145643.4223554655\n",
      "Current iteration=294, loss=145632.50155864208\n",
      "Current iteration=295, loss=145621.63383980189\n",
      "Current iteration=296, loss=145610.81867999758\n",
      "Current iteration=297, loss=145600.05556735562\n",
      "Current iteration=298, loss=145589.34399696067\n",
      "Current iteration=299, loss=145578.68347074222\n",
      "Current iteration=300, loss=145568.07349736334\n",
      "Current iteration=301, loss=145557.51359211138\n",
      "Current iteration=302, loss=145547.0032767908\n",
      "Current iteration=303, loss=145536.54207961797\n",
      "Current iteration=304, loss=145526.12953511774\n",
      "Current iteration=305, loss=145515.76518402217\n",
      "Current iteration=306, loss=145505.44857317096\n",
      "Current iteration=307, loss=145495.17925541362\n",
      "Current iteration=308, loss=145484.95678951373\n",
      "Current iteration=309, loss=145474.78074005453\n",
      "Current iteration=310, loss=145464.6506773466\n",
      "Current iteration=311, loss=145454.5661773369\n",
      "Current iteration=312, loss=145444.5268215198\n",
      "Current iteration=313, loss=145434.5321968493\n",
      "Current iteration=314, loss=145424.58189565325\n",
      "Current iteration=315, loss=145414.67551554876\n",
      "Current iteration=316, loss=145404.81265935936\n",
      "Current iteration=317, loss=145394.99293503357\n",
      "Current iteration=318, loss=145385.21595556487\n",
      "Current iteration=319, loss=145375.48133891312\n",
      "Current iteration=320, loss=145365.7887079275\n",
      "Current iteration=321, loss=145356.13769027064\n",
      "Current iteration=322, loss=145346.52791834416\n",
      "Current iteration=323, loss=145336.95902921556\n",
      "Current iteration=324, loss=145327.4306645464\n",
      "Current iteration=325, loss=145317.94247052164\n",
      "Current iteration=326, loss=145308.49409778038\n",
      "Current iteration=327, loss=145299.08520134765\n",
      "Current iteration=328, loss=145289.71544056755\n",
      "Current iteration=329, loss=145280.38447903746\n",
      "Current iteration=330, loss=145271.09198454334\n",
      "Current iteration=331, loss=145261.83762899635\n",
      "Current iteration=332, loss=145252.62108837036\n",
      "Current iteration=333, loss=145243.44204264064\n",
      "Current iteration=334, loss=145234.30017572362\n",
      "Current iteration=335, loss=145225.19517541773\n",
      "Current iteration=336, loss=145216.12673334504\n",
      "Current iteration=337, loss=145207.09454489415\n",
      "Current iteration=338, loss=145198.09830916402\n",
      "Current iteration=339, loss=145189.13772890856\n",
      "Current iteration=340, loss=145180.21251048247\n",
      "Current iteration=341, loss=145171.3223637877\n",
      "Current iteration=342, loss=145162.4670022211\n",
      "Current iteration=343, loss=145153.6461426226\n",
      "Current iteration=344, loss=145144.85950522477\n",
      "Current iteration=345, loss=145136.10681360276\n",
      "Current iteration=346, loss=145127.38779462533\n",
      "Current iteration=347, loss=145118.70217840662\n",
      "Current iteration=348, loss=145110.04969825875\n",
      "Current iteration=349, loss=145101.4300906453\n",
      "Current iteration=350, loss=145092.84309513538\n",
      "Current iteration=351, loss=145084.28845435867\n",
      "Current iteration=352, loss=145075.76591396105\n",
      "Current iteration=353, loss=145067.275222561\n",
      "Current iteration=354, loss=145058.81613170693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=355, loss=145050.38839583477\n",
      "Current iteration=356, loss=145041.9917722268\n",
      "Current iteration=357, loss=145033.62602097064\n",
      "Current iteration=358, loss=145025.29090491947\n",
      "Current iteration=359, loss=145016.98618965226\n",
      "Current iteration=360, loss=145008.71164343526\n",
      "Current iteration=361, loss=145000.46703718373\n",
      "Current iteration=362, loss=144992.2521444244\n",
      "Current iteration=363, loss=144984.06674125863\n",
      "Current iteration=364, loss=144975.91060632607\n",
      "Current iteration=365, loss=144967.78352076898\n",
      "Current iteration=366, loss=144959.68526819692\n",
      "Current iteration=367, loss=144951.61563465244\n",
      "Current iteration=368, loss=144943.57440857685\n",
      "Current iteration=369, loss=144935.56138077687\n",
      "Current iteration=370, loss=144927.57634439162\n",
      "Current iteration=371, loss=144919.61909486033\n",
      "Current iteration=372, loss=144911.68942989028\n",
      "Current iteration=373, loss=144903.78714942568\n",
      "Current iteration=374, loss=144895.9120556166\n",
      "Current iteration=375, loss=144888.0639527886\n",
      "Current iteration=376, loss=144880.242647413\n",
      "Current iteration=377, loss=144872.44794807723\n",
      "Current iteration=378, loss=144864.67966545612\n",
      "Current iteration=379, loss=144856.9376122832\n",
      "Current iteration=380, loss=144849.22160332275\n",
      "Current iteration=381, loss=144841.53145534213\n",
      "Current iteration=382, loss=144833.86698708477\n",
      "Current iteration=383, loss=144826.22801924322\n",
      "Current iteration=384, loss=144818.61437443292\n",
      "Current iteration=385, loss=144811.0258771663\n",
      "Current iteration=386, loss=144803.4623538273\n",
      "Current iteration=387, loss=144795.9236326461\n",
      "Current iteration=388, loss=144788.40954367458\n",
      "Current iteration=389, loss=144780.91991876197\n",
      "Current iteration=390, loss=144773.45459153067\n",
      "Current iteration=391, loss=144766.01339735292\n",
      "Current iteration=392, loss=144758.5961733275\n",
      "Current iteration=393, loss=144751.20275825672\n",
      "Current iteration=394, loss=144743.8329926241\n",
      "Current iteration=395, loss=144736.4867185721\n",
      "Current iteration=396, loss=144729.16377988024\n",
      "Current iteration=397, loss=144721.86402194377\n",
      "Current iteration=398, loss=144714.58729175222\n",
      "Current iteration=399, loss=144707.3334378689\n",
      "Current iteration=400, loss=144700.10231041003\n",
      "Current iteration=401, loss=144692.8937610248\n",
      "Current iteration=402, loss=144685.70764287526\n",
      "Current iteration=403, loss=144678.54381061683\n",
      "Current iteration=404, loss=144671.4021203789\n",
      "Current iteration=405, loss=144664.2824297458\n",
      "Current iteration=406, loss=144657.18459773817\n",
      "Current iteration=407, loss=144650.10848479444\n",
      "Current iteration=408, loss=144643.05395275255\n",
      "Current iteration=409, loss=144636.0208648322\n",
      "Current iteration=410, loss=144629.0090856171\n",
      "Current iteration=411, loss=144622.0184810376\n",
      "Current iteration=412, loss=144615.0489183537\n",
      "Current iteration=413, loss=144608.10026613798\n",
      "Current iteration=414, loss=144601.17239425908\n",
      "Current iteration=415, loss=144594.2651738653\n",
      "Current iteration=416, loss=144587.37847736862\n",
      "Current iteration=417, loss=144580.51217842856\n",
      "Current iteration=418, loss=144573.66615193666\n",
      "Current iteration=419, loss=144566.84027400106\n",
      "Current iteration=420, loss=144560.03442193125\n",
      "Current iteration=421, loss=144553.24847422316\n",
      "Current iteration=422, loss=144546.4823105443\n",
      "Current iteration=423, loss=144539.7358117192\n",
      "Current iteration=424, loss=144533.00885971525\n",
      "Current iteration=425, loss=144526.30133762842\n",
      "Current iteration=426, loss=144519.61312966933\n",
      "Current iteration=427, loss=144512.94412114963\n",
      "Current iteration=428, loss=144506.29419846836\n",
      "Current iteration=429, loss=144499.66324909875\n",
      "Current iteration=430, loss=144493.05116157493\n",
      "Current iteration=431, loss=144486.45782547918\n",
      "Current iteration=432, loss=144479.88313142894\n",
      "Current iteration=433, loss=144473.3269710644\n",
      "Current iteration=434, loss=144466.7892370361\n",
      "Current iteration=435, loss=144460.26982299256\n",
      "Current iteration=436, loss=144453.76862356835\n",
      "Current iteration=437, loss=144447.2855343722\n",
      "Current iteration=438, loss=144440.82045197528\n",
      "Current iteration=439, loss=144434.37327389963\n",
      "Current iteration=440, loss=144427.9438986068\n",
      "Current iteration=441, loss=144421.5322254867\n",
      "Current iteration=442, loss=144415.13815484638\n",
      "Current iteration=443, loss=144408.76158789924\n",
      "Current iteration=444, loss=144402.40242675427\n",
      "Current iteration=445, loss=144396.06057440536\n",
      "Current iteration=446, loss=144389.735934721\n",
      "Current iteration=447, loss=144383.4284124338\n",
      "Current iteration=448, loss=144377.1379131305\n",
      "Current iteration=449, loss=144370.86434324167\n",
      "Current iteration=450, loss=144364.60761003222\n",
      "Current iteration=451, loss=144358.36762159131\n",
      "Current iteration=452, loss=144352.14428682288\n",
      "Current iteration=453, loss=144345.93751543612\n",
      "Current iteration=454, loss=144339.7472179362\n",
      "Current iteration=455, loss=144333.57330561502\n",
      "Current iteration=456, loss=144327.415690542\n",
      "Current iteration=457, loss=144321.27428555526\n",
      "Current iteration=458, loss=144315.14900425271\n",
      "Current iteration=459, loss=144309.03976098332\n",
      "Current iteration=460, loss=144302.94647083848\n",
      "Current iteration=461, loss=144296.86904964363\n",
      "Current iteration=462, loss=144290.80741394975\n",
      "Current iteration=463, loss=144284.76148102508\n",
      "Current iteration=464, loss=144278.73116884715\n",
      "Current iteration=465, loss=144272.71639609453\n",
      "Current iteration=466, loss=144266.7170821391\n",
      "Current iteration=467, loss=144260.73314703797\n",
      "Current iteration=468, loss=144254.76451152604\n",
      "Current iteration=469, loss=144248.81109700818\n",
      "Current iteration=470, loss=144242.8728255518\n",
      "Current iteration=471, loss=144236.9496198794\n",
      "Current iteration=472, loss=144231.04140336125\n",
      "Current iteration=473, loss=144225.14810000831\n",
      "Current iteration=474, loss=144219.26963446484\n",
      "Current iteration=475, loss=144213.40593200162\n",
      "Current iteration=476, loss=144207.5569185089\n",
      "Current iteration=477, loss=144201.72252048954\n",
      "Current iteration=478, loss=144195.9026650523\n",
      "Current iteration=479, loss=144190.0972799052\n",
      "Current iteration=480, loss=144184.30629334887\n",
      "Current iteration=481, loss=144178.52963427015\n",
      "Current iteration=482, loss=144172.76723213552\n",
      "Current iteration=483, loss=144167.01901698505\n",
      "Current iteration=484, loss=144161.28491942587\n",
      "Current iteration=485, loss=144155.5648706262\n",
      "Current iteration=486, loss=144149.85880230923\n",
      "Current iteration=487, loss=144144.16664674703\n",
      "Current iteration=488, loss=144138.48833675482\n",
      "Current iteration=489, loss=144132.8238056849\n",
      "Current iteration=490, loss=144127.17298742104\n",
      "Current iteration=491, loss=144121.53581637275\n",
      "Current iteration=492, loss=144115.9122274696\n",
      "Current iteration=493, loss=144110.30215615578\n",
      "Current iteration=494, loss=144104.70553838447\n",
      "Current iteration=495, loss=144099.12231061255\n",
      "Current iteration=496, loss=144093.5524097953\n",
      "Current iteration=497, loss=144087.9957733809\n",
      "Current iteration=498, loss=144082.45233930554\n",
      "Current iteration=499, loss=144076.92204598803\n",
      "Current iteration=500, loss=144071.40483232483\n",
      "Current iteration=501, loss=144065.9006376851\n",
      "Current iteration=502, loss=144060.40940190552\n",
      "Current iteration=503, loss=144054.93106528578\n",
      "Current iteration=504, loss=144049.46556858343\n",
      "Current iteration=505, loss=144044.01285300922\n",
      "Current iteration=506, loss=144038.5728602225\n",
      "Current iteration=507, loss=144033.14553232642\n",
      "Current iteration=508, loss=144027.73081186347\n",
      "Current iteration=509, loss=144022.32864181086\n",
      "Current iteration=510, loss=144016.93896557612\n",
      "Current iteration=511, loss=144011.56172699266\n",
      "Current iteration=512, loss=144006.19687031538\n",
      "Current iteration=513, loss=144000.84434021643\n",
      "Current iteration=514, loss=143995.50408178082\n",
      "Current iteration=515, loss=143990.17604050244\n",
      "Current iteration=516, loss=143984.86016227969\n",
      "Current iteration=517, loss=143979.5563934115\n",
      "Current iteration=518, loss=143974.2646805933\n",
      "Current iteration=519, loss=143968.98497091292\n",
      "Current iteration=520, loss=143963.71721184673\n",
      "Current iteration=521, loss=143958.46135125568\n",
      "Current iteration=522, loss=143953.21733738153\n",
      "Current iteration=523, loss=143947.98511884292\n",
      "Current iteration=524, loss=143942.76464463174\n",
      "Current iteration=525, loss=143937.55586410928\n",
      "Current iteration=526, loss=143932.3587270027\n",
      "Current iteration=527, loss=143927.17318340132\n",
      "Current iteration=528, loss=143921.999183753\n",
      "Current iteration=529, loss=143916.83667886074\n",
      "Current iteration=530, loss=143911.68561987905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=531, loss=143906.5459583105\n",
      "Current iteration=532, loss=143901.41764600246\n",
      "Current iteration=533, loss=143896.3006351435\n",
      "Current iteration=534, loss=143891.19487826023\n",
      "Current iteration=535, loss=143886.100328214\n",
      "Current iteration=536, loss=143881.0169381975\n",
      "Current iteration=537, loss=143875.94466173172\n",
      "Current iteration=538, loss=143870.88345266265\n",
      "Current iteration=539, loss=143865.83326515823\n",
      "Current iteration=540, loss=143860.79405370515\n",
      "Current iteration=541, loss=143855.7657731059\n",
      "Current iteration=542, loss=143850.74837847566\n",
      "Current iteration=543, loss=143845.74182523927\n",
      "Current iteration=544, loss=143840.74606912842\n",
      "Current iteration=545, loss=143835.76106617856\n",
      "Current iteration=546, loss=143830.7867727261\n",
      "Current iteration=547, loss=143825.8231454056\n",
      "Current iteration=548, loss=143820.87014114673\n",
      "Current iteration=549, loss=143815.92771717175\n",
      "Current iteration=550, loss=143810.99583099264\n",
      "Current iteration=551, loss=143806.07444040832\n",
      "Current iteration=552, loss=143801.163503502\n",
      "Current iteration=553, loss=143796.2629786385\n",
      "Current iteration=554, loss=143791.3728244617\n",
      "Current iteration=555, loss=143786.49299989178\n",
      "Current iteration=556, loss=143781.62346412282\n",
      "Current iteration=557, loss=143776.7641766201\n",
      "Current iteration=558, loss=143771.91509711777\n",
      "Current iteration=559, loss=143767.07618561608\n",
      "Current iteration=560, loss=143762.24740237926\n",
      "Current iteration=561, loss=143757.42870793282\n",
      "Current iteration=562, loss=143752.6200630613\n",
      "Current iteration=563, loss=143747.8214288058\n",
      "Current iteration=564, loss=143743.03276646175\n",
      "Current iteration=565, loss=143738.25403757644\n",
      "Current iteration=566, loss=143733.48520394685\n",
      "Current iteration=567, loss=143728.72622761733\n",
      "Current iteration=568, loss=143723.97707087733\n",
      "Current iteration=569, loss=143719.2376962592\n",
      "Current iteration=570, loss=143714.50806653601\n",
      "Current iteration=571, loss=143709.78814471944\n",
      "Current iteration=572, loss=143705.07789405747\n",
      "Current iteration=573, loss=143700.3772780324\n",
      "Current iteration=574, loss=143695.6862603587\n",
      "Current iteration=575, loss=143691.00480498088\n",
      "Current iteration=576, loss=143686.33287607165\n",
      "Current iteration=577, loss=143681.6704380296\n",
      "Current iteration=578, loss=143677.01745547738\n",
      "Current iteration=579, loss=143672.37389325968\n",
      "Current iteration=580, loss=143667.7397164412\n",
      "Current iteration=581, loss=143663.1148903048\n",
      "Current iteration=582, loss=143658.49938034956\n",
      "Current iteration=583, loss=143653.8931522888\n",
      "Current iteration=584, loss=143649.29617204834\n",
      "Current iteration=585, loss=143644.7084057645\n",
      "Current iteration=586, loss=143640.12981978233\n",
      "Current iteration=587, loss=143635.56038065386\n",
      "Current iteration=588, loss=143631.0000551361\n",
      "Current iteration=589, loss=143626.44881018947\n",
      "Current iteration=590, loss=143621.906612976\n",
      "Current iteration=591, loss=143617.3734308574\n",
      "Current iteration=592, loss=143612.84923139366\n",
      "Current iteration=593, loss=143608.33398234108\n",
      "Current iteration=594, loss=143603.82765165062\n",
      "Current iteration=595, loss=143599.3302074664\n",
      "Current iteration=596, loss=143594.84161812387\n",
      "Current iteration=597, loss=143590.3618521483\n",
      "Current iteration=598, loss=143585.890878253\n",
      "Current iteration=599, loss=143581.42866533797\n",
      "Current iteration=600, loss=143576.9751824881\n",
      "Current iteration=601, loss=143572.53039897166\n",
      "Current iteration=602, loss=143568.09428423888\n",
      "Current iteration=603, loss=143563.66680792024\n",
      "Current iteration=604, loss=143559.24793982503\n",
      "Current iteration=605, loss=143554.83764993987\n",
      "Current iteration=606, loss=143550.4359084273\n",
      "Current iteration=607, loss=143546.042685624\n",
      "Current iteration=608, loss=143541.65795203982\n",
      "Current iteration=609, loss=143537.2816783559\n",
      "Current iteration=610, loss=143532.9138354235\n",
      "Current iteration=611, loss=143528.55439426246\n",
      "Current iteration=612, loss=143524.20332606\n",
      "Current iteration=613, loss=143519.86060216906\n",
      "Current iteration=614, loss=143515.52619410714\n",
      "Current iteration=615, loss=143511.20007355494\n",
      "Current iteration=616, loss=143506.8822123549\n",
      "Current iteration=617, loss=143502.57258251\n",
      "Current iteration=618, loss=143498.2711561824\n",
      "Current iteration=619, loss=143493.97790569213\n",
      "Current iteration=620, loss=143489.69280351585\n",
      "Current iteration=621, loss=143485.41582228555\n",
      "Current iteration=622, loss=143481.14693478725\n",
      "Current iteration=623, loss=143476.8861139599\n",
      "Current iteration=624, loss=143472.63333289398\n",
      "Current iteration=625, loss=143468.38856483035\n",
      "Current iteration=626, loss=143464.15178315915\n",
      "Current iteration=627, loss=143459.92296141837\n",
      "Current iteration=628, loss=143455.7020732929\n",
      "Current iteration=629, loss=143451.4890926132\n",
      "Current iteration=630, loss=143447.2839933542\n",
      "Current iteration=631, loss=143443.0867496343\n",
      "Current iteration=632, loss=143438.8973357139\n",
      "Current iteration=633, loss=143434.71572599455\n",
      "Current iteration=634, loss=143430.54189501778\n",
      "Current iteration=635, loss=143426.3758174639\n",
      "Current iteration=636, loss=143422.2174681511\n",
      "Current iteration=637, loss=143418.06682203413\n",
      "Current iteration=638, loss=143413.9238542034\n",
      "Current iteration=639, loss=143409.78853988386\n",
      "Current iteration=640, loss=143405.66085443404\n",
      "Current iteration=641, loss=143401.54077334484\n",
      "Current iteration=642, loss=143397.4282722386\n",
      "Current iteration=643, loss=143393.3233268682\n",
      "Current iteration=644, loss=143389.22591311584\n",
      "Current iteration=645, loss=143385.13600699208\n",
      "Current iteration=646, loss=143381.05358463505\n",
      "Current iteration=647, loss=143376.97862230922\n",
      "Current iteration=648, loss=143372.91109640463\n",
      "Current iteration=649, loss=143368.85098343572\n",
      "Current iteration=650, loss=143364.79826004067\n",
      "Current iteration=651, loss=143360.7529029801\n",
      "Current iteration=652, loss=143356.71488913646\n",
      "Current iteration=653, loss=143352.68419551296\n",
      "Current iteration=654, loss=143348.66079923263\n",
      "Current iteration=655, loss=143344.64467753746\n",
      "Current iteration=656, loss=143340.63580778753\n",
      "Current iteration=657, loss=143336.63416746008\n",
      "Current iteration=658, loss=143332.63973414866\n",
      "Current iteration=659, loss=143328.6524855622\n",
      "Current iteration=660, loss=143324.6723995242\n",
      "Current iteration=661, loss=143320.6994539719\n",
      "Current iteration=662, loss=143316.73362695533\n",
      "Current iteration=663, loss=143312.77489663663\n",
      "Current iteration=664, loss=143308.82324128906\n",
      "Current iteration=665, loss=143304.87863929625\n",
      "Current iteration=666, loss=143300.94106915142\n",
      "Current iteration=667, loss=143297.01050945654\n",
      "Current iteration=668, loss=143293.08693892142\n",
      "Current iteration=669, loss=143289.17033636314\n",
      "Current iteration=670, loss=143285.2606807051\n",
      "Current iteration=671, loss=143281.35795097632\n",
      "Current iteration=672, loss=143277.4621263106\n",
      "Current iteration=673, loss=143273.57318594577\n",
      "Current iteration=674, loss=143269.69110922312\n",
      "Current iteration=675, loss=143265.81587558627\n",
      "Current iteration=676, loss=143261.94746458082\n",
      "Current iteration=677, loss=143258.08585585337\n",
      "Current iteration=678, loss=143254.23102915086\n",
      "Current iteration=679, loss=143250.38296431993\n",
      "Current iteration=680, loss=143246.54164130607\n",
      "Current iteration=681, loss=143242.707040153\n",
      "Current iteration=682, loss=143238.8791410019\n",
      "Current iteration=683, loss=143235.0579240908\n",
      "Current iteration=684, loss=143231.24336975385\n",
      "Current iteration=685, loss=143227.4354584206\n",
      "Current iteration=686, loss=143223.6341706154\n",
      "Current iteration=687, loss=143219.83948695668\n",
      "Current iteration=688, loss=143216.05138815625\n",
      "Current iteration=689, loss=143212.26985501873\n",
      "Current iteration=690, loss=143208.49486844084\n",
      "Current iteration=691, loss=143204.7264094108\n",
      "Current iteration=692, loss=143200.9644590076\n",
      "Current iteration=693, loss=143197.20899840049\n",
      "Current iteration=694, loss=143193.4600088482\n",
      "Current iteration=695, loss=143189.71747169847\n",
      "Current iteration=696, loss=143185.98136838735\n",
      "Current iteration=697, loss=143182.25168043855\n",
      "Current iteration=698, loss=143178.528389463\n",
      "Current iteration=699, loss=143174.811477158\n",
      "Current iteration=700, loss=143171.10092530682\n",
      "Current iteration=701, loss=143167.39671577807\n",
      "Current iteration=702, loss=143163.69883052504\n",
      "Current iteration=703, loss=143160.0072515852\n",
      "Current iteration=704, loss=143156.32196107955\n",
      "Current iteration=705, loss=143152.6429412122\n",
      "Current iteration=706, loss=143148.97017426958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=707, loss=143145.3036426201\n",
      "Current iteration=708, loss=143141.6433287134\n",
      "Current iteration=709, loss=143137.98921508\n",
      "Current iteration=710, loss=143134.34128433053\n",
      "Current iteration=711, loss=143130.69951915543\n",
      "Current iteration=712, loss=143127.06390232418\n",
      "Current iteration=713, loss=143123.434416685\n",
      "Current iteration=714, loss=143119.8110451641\n",
      "Current iteration=715, loss=143116.19377076533\n",
      "Current iteration=716, loss=143112.58257656955\n",
      "Current iteration=717, loss=143108.9774457342\n",
      "Current iteration=718, loss=143105.37836149274\n",
      "Current iteration=719, loss=143101.78530715412\n",
      "Current iteration=720, loss=143098.19826610244\n",
      "Current iteration=721, loss=143094.61722179616\n",
      "Current iteration=722, loss=143091.04215776792\n",
      "Current iteration=723, loss=143087.4730576239\n",
      "Current iteration=724, loss=143083.90990504326\n",
      "Current iteration=725, loss=143080.35268377786\n",
      "Current iteration=726, loss=143076.8013776516\n",
      "Current iteration=727, loss=143073.25597056013\n",
      "Current iteration=728, loss=143069.71644647012\n",
      "Current iteration=729, loss=143066.18278941914\n",
      "Current iteration=730, loss=143062.65498351492\n",
      "Current iteration=731, loss=143059.13301293497\n",
      "Current iteration=732, loss=143055.61686192622\n",
      "Current iteration=733, loss=143052.10651480447\n",
      "Current iteration=734, loss=143048.60195595401\n",
      "Current iteration=735, loss=143045.10316982714\n",
      "Current iteration=736, loss=143041.61014094373\n",
      "Current iteration=737, loss=143038.12285389085\n",
      "Current iteration=738, loss=143034.6412933222\n",
      "Current iteration=739, loss=143031.1654439579\n",
      "Current iteration=740, loss=143027.69529058388\n",
      "Current iteration=741, loss=143024.23081805152\n",
      "Current iteration=742, loss=143020.77201127732\n",
      "Current iteration=743, loss=143017.31885524234\n",
      "Current iteration=744, loss=143013.8713349919\n",
      "Current iteration=745, loss=143010.4294356351\n",
      "Current iteration=746, loss=143006.99314234455\n",
      "Current iteration=747, loss=143003.56244035577\n",
      "Current iteration=748, loss=143000.137314967\n",
      "Current iteration=749, loss=142996.71775153867\n",
      "Current iteration=750, loss=142993.30373549307\n",
      "Current iteration=751, loss=142989.89525231396\n",
      "Current iteration=752, loss=142986.4922875462\n",
      "Current iteration=753, loss=142983.09482679525\n",
      "Current iteration=754, loss=142979.70285572705\n",
      "Current iteration=755, loss=142976.3163600674\n",
      "Current iteration=756, loss=142972.93532560172\n",
      "Current iteration=757, loss=142969.5597381746\n",
      "Current iteration=758, loss=142966.18958368953\n",
      "Current iteration=759, loss=142962.82484810852\n",
      "Current iteration=760, loss=142959.4655174516\n",
      "Current iteration=761, loss=142956.11157779675\n",
      "Current iteration=762, loss=142952.76301527926\n",
      "Current iteration=763, loss=142949.41981609148\n",
      "Current iteration=764, loss=142946.08196648263\n",
      "Current iteration=765, loss=142942.74945275814\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_gradient_descent_demo(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = ridge_regression(lambda_=0.00067233575365, tx=px_train, y=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict_labels(ws, px_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = 1 - np.mean( y_pred != test_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79814%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" + str(accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(idstest, y_pred, 'prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degree = 6\n",
    "k_fold = 10\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "best_loss = 999\n",
    "for k in range(k_fold):\n",
    "    temp_tr = []\n",
    "    temp_te = []\n",
    "    best_test_ws = []\n",
    "    for lambda_ in lambdas:\n",
    "        tr_loss, te_loss, ws = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "        if(te_loss < best_loss):\n",
    "            best_loss = te_loss\n",
    "            best_lambda = lambda_\n",
    "        temp_tr.append(tr_loss)\n",
    "        temp_te.append(te_loss)\n",
    "        # print(\"Lambda = \" + str(lambda_) + \" tr_loss = \" + str(tr_loss) + \" te_loss = \" + str(te_loss))\n",
    "    print(\"After lambdas iteration, the best lambda is : \" + str(best_lambda) + \" for k-fold : \" + str(k) + \" with best loss = \" + str(best_loss))\n",
    "    best_test_ws.append(lambda_)\n",
    "    rmse_tr.append(temp_tr)\n",
    "    rmse_te.append(temp_te)\n",
    "\n",
    "rmse_tr = np.matrix(rmse_tr)\n",
    "rmse_tr = np.mean(rmse_tr, axis=0)\n",
    "rmse_tr = np.reshape(rmse_tr, (len(lambdas),-1))\n",
    "rmse_te = np.matrix(rmse_te)\n",
    "rmse_te = np.mean(rmse_te, axis=0)\n",
    "rmse_te = np.reshape(rmse_te, (len(lambdas),-1))\n",
    "\n",
    "cross_validation_visualization(lambdas, rmse_tr, rmse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # return loss, gradient and hessian: TODO\n",
    "    # ***************************************************\n",
    "    print('here it is')\n",
    "    loss, gradient, hessian = logistic_regression(y,tx,w)\n",
    "    \n",
    "    #raise NotImplementedError\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # update w: TODO\n",
    "    # ***************************************************\n",
    "    print('here it is')\n",
    "    termine = np.linalg.solve(hessian,gradient)\n",
    "    w = w-termine\n",
    "    \n",
    "    #raise NotImplementedError\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.01\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((x.shape[1], ))\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, x, w)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_newton_method\")\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logistic_regression_newton_method_demo(y,x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
