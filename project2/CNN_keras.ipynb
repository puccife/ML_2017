{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/mezo/anaconda3/envs/ada/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, Merge, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from utils.preprocessing import clean_tweets\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- Parameters section -------------------\n",
    "#\n",
    "# Model type. See Kim Yoon's Convolutional Neural Networks for Sentence Classification, Section 3\n",
    "model_type = \"CNN-non-static\"  # CNN-rand|CNN-non-static|CNN-static\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 200\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 200\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "#\n",
    "# ---------------------- Parameters end -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(\"./twitter-datasets/train_pos.txt\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(\"./twitter-datasets/train_neg.txt\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_tweets(sent) for sent in x_text]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentences(sentences, padding_word=\"padding_word\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = 20\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        if num_padding < 0:\n",
    "            new_sentence = sentence[:sequence_length]\n",
    "        else:\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "        \n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_word_embeddings():\n",
    "    embeddings_index = {}\n",
    "    print('Indexing word vectors.')\n",
    "    f = open(os.path.join('./glove.twitter.27B/', 'glove.twitter.27B.'+str(200)+'d.txt'))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manipulate_dataset(dataset,word_embeddings):\n",
    "    missing_voc={}\n",
    "    output_array = np.ndarray((len(dataset),20,200))\n",
    "    for i,sentence in enumerate(dataset):\n",
    "        matrix_embedding = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                matrix_embedding.append(word_embeddings[word])\n",
    "            except:\n",
    "                vector = create_vector(word,word_embeddings,200,silent=True)\n",
    "                matrix_embedding.append(vector)\n",
    "                try:\n",
    "                    missing_voc[word] = missing_voc[word] + 1\n",
    "                except KeyError:\n",
    "                    missing_voc[word] = 1\n",
    "        output_array[i]=(matrix_embedding)\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vector(word, word_embeddings, word_vector_size, silent=True):\n",
    "    # if the word is missing from Glove or Google Vectors, create some fake vector and store in glove!\n",
    "    vector = np.random.uniform(0.0, 1.0, (word_vector_size,))\n",
    "    word_embeddings[word] = vector\n",
    "    if not silent:\n",
    "        print(\"utils.py::create_vector => %s is missing\" % word)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_and_shuffle(x,y, ratio, seed):\n",
    "    split_index = int(len(x)*ratio)\n",
    "    train_x, test_x = x[:split_index], x[split_index:]\n",
    "    train_y, test_y = y[:split_index], y[split_index:]\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(train_x)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(test_x)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(train_y)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(test_y)\n",
    "    return train_x,test_x,train_y,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = load_data_and_labels()\n",
    "sentences_padded = pad_sentences(sentences)\n",
    "x = sentences_padded\n",
    "y = np.array(labels)\n",
    "#vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n",
    "y = y.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = split_and_shuffle(x,y,0.9,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train shape:\", len(x_train))\n",
    "print(\"x_test shape:\", len(x_test))\n",
    "print(\"y_train shape:\", len(y_train))\n",
    "print(\"y_test shape:\", len(y_test))\n",
    "#print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_words = generate_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "#     sentences, labels = load_data_and_labels()\n",
    "#     sentences_padded = pad_sentences(sentences)\n",
    "#     x = sentences_padded\n",
    "#     y = np.array(labels)\n",
    "#     y = y.argmax(axis=1)\n",
    "#     x_train,x_test,y_train,y_test = split_and_shuffle(x,y,0.9,33)\n",
    "    #embeddings_words = generate_word_embeddings()\n",
    "    \n",
    "    while 1:\n",
    "        for i in range(int(len(x_train)/10000)): \n",
    "            x_train_embeddings = manipulate_dataset(x_train[i*10000:((i+1)*10000)],embeddings_words)\n",
    "            #x_test_embeddings = manipulate_dataset(x_test,embeddings_words)\n",
    "            if i%10000==0:\n",
    "                print (\"i = \" + str(i))\n",
    "                print(x_train_embeddings.shape)\n",
    "                print(y_train.shape)\n",
    "            yield x_train_embeddings, y_train[(i*10000):((i+1)*10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_validator():\n",
    "#     sentences, labels = load_data_and_labels()\n",
    "#     sentences_padded = pad_sentences(sentences)\n",
    "#     x = sentences_padded\n",
    "#     y = np.array(labels)\n",
    "#     y = y.argmax(axis=1)\n",
    "#     x_train,x_test,y_train,y_test = split_and_shuffle(x,y,0.9,33)\n",
    "    #embeddings_words = generate_word_embeddings()\n",
    "    \n",
    "    while 1:\n",
    "        for j in range(int(len(x_test)/10000)): \n",
    "            #x_train_embeddings = manipulate_dataset(x_train[i*10000:((i+1)*10000)],embeddings_words)\n",
    "            x_test_embeddings = manipulate_dataset(x_test[j*10000:((j+1)*10000)],embeddings_words)\n",
    "            if j%10000==0:\n",
    "                print (\"j = \" + str(j))\n",
    "            yield x_test_embeddings, y_test[(j*10000):((j+1)*10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_embeddings = manipulate_dataset(x_train[0,embeddings_words)\n",
    "x_test_embeddings = manipulate_dataset(x_test,embeddings_words)\n",
    "\n",
    "\n",
    "#x_train = np.stack([np.stack([embeddings_idnex[word] for word in sentence]) for sentence in x_train])\n",
    "#x_test = np.stack([np.stack([embeddings_idnex[word] for word in sentence]) for sentence in x_test])\n",
    "#print(\"x_train static shape:\", x_train.shape)\n",
    "#print(\"x_test static shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = (20, 200)\n",
    "model_input = Input(shape=input_shape)\n",
    "z = model_input\n",
    "z = Dropout(dropout_prob[0])(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator(),steps_per_epoch=180000, epochs=num_epochs,validation_data= generator_validator(),validation_steps=20000, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_test():\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    x_text = list(open(\"./twitter-datasets/test_data.txt\").readlines())\n",
    "    x_text = [s.strip() for s in x_text]\n",
    "    # Split by words\n",
    "    x_text = [clean_tweets(sent) for sent in x_text]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "    \n",
    "    return x_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_text = load_data_test()\n",
    "x_test_text_padded = pad_sentences(x_test_text)\n",
    "embeddings_words = generate_word_embeddings()\n",
    "x_test_embeddings = manipulate_dataset(x_test_text_padded,embeddings_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('my_first_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(x_test_embeddings, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
