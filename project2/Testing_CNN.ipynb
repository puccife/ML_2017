{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from utils.preprocessing import clean_tweets\n",
    "import os\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_test():\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    x_text = list(open(\"./twitter-datasets/test_data.txt\").readlines())\n",
    "    x_text = [s.strip() for s in x_text]\n",
    "    # Split by words\n",
    "    x_text = [clean_tweets(sent) for sent in x_text]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "    \n",
    "    return x_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vector(word, word_embeddings, word_vector_size, silent=True):\n",
    "    # if the word is missing from Glove or Google Vectors, create some fake vector and store in glove!\n",
    "    vector = np.random.uniform(0.0, 1.0, (word_vector_size,))\n",
    "    word_embeddings[word] = vector\n",
    "    if not silent:\n",
    "        print(\"utils.py::create_vector => %s is missing\" % word)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manipulate_dataset(dataset,word_embeddings):\n",
    "    missing_voc={}\n",
    "    output_array = np.ndarray((len(dataset),20,200))\n",
    "    for i,sentence in enumerate(dataset):\n",
    "        matrix_embedding = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                matrix_embedding.append(word_embeddings[word])\n",
    "            except:\n",
    "                vector = create_vector(word,word_embeddings,200,silent=True)\n",
    "                matrix_embedding.append(vector)\n",
    "                try:\n",
    "                    missing_voc[word] = missing_voc[word] + 1\n",
    "                except KeyError:\n",
    "                    missing_voc[word] = 1\n",
    "        output_array[i]=(matrix_embedding)\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_word_embeddings():\n",
    "    embeddings_index = {}\n",
    "    print('Indexing word vectors.')\n",
    "    f = open(os.path.join('./glove.twitter.27B/', 'glove.twitter.27B.'+str(200)+'d.txt'))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentences(sentences, padding_word=\"padding_word\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = 20\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        if num_padding < 0:\n",
    "            new_sentence = sentence[:sequence_length]\n",
    "        else:\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "        \n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_text = load_data_test()\n",
    "x_test_text_padded = pad_sentences(x_test_text)\n",
    "embeddings_words = generate_word_embeddings()\n",
    "x_test_embeddings = manipulate_dataset(x_test_text_padded,embeddings_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('my_first_model.h5')\n",
    "y_output = model.predict(x_test_embeddings, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
